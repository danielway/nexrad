#![cfg(all(feature = "aws-polling", not(target_arch = "wasm32")))]

use chrono::{DateTime, SubsecRound, Utc};
use clap::Parser;
use env_logger::{Builder, Env};
use futures::StreamExt;
use log::{debug, info, warn, LevelFilter};
use nexrad_data::result::Result;
use nexrad_data::{
    aws::realtime::{self, chunk_stream, Chunk, DownloadedChunk, PollConfig},
    volume,
};
use nexrad_decode::summarize;
use std::pin::pin;
use std::time::Duration;
use tokio::time::timeout;

// Example designed to provide concise latency analysis for NEXRAD data chunks
// Output format (single line per chunk):
// Chunk: <name> | Downloaded: <time> | AWS Latency: <value>s | First Radial Latency: <value>s | Last Radial Latency: <value>s | Attempts: <count>

#[derive(Parser)]
#[command(author, version, about, long_about = None)]
struct Cli {
    /// Site identifier (e.g., KDMX)
    #[arg(default_value = "KDMX")]
    site: String,

    /// The number of chunks to download
    #[arg(default_value = "10")]
    chunk_count: usize,
}

#[tokio::main]
async fn main() -> Result<()> {
    Builder::from_env(Env::default().default_filter_or("info"))
        .filter_module("reqwest::connect", LevelFilter::Info)
        .init();

    let cli = Cli::parse();
    let desired_chunk_count = cli.chunk_count;

    println!(
        "{:<25} | {:<25} | {:<13} | {:<15} {:<29} | {:<8}",
        "", "", "Time Since", "", "Latency Since", ""
    );
    println!(
        "{:<25} | {:<25} | {:<13} | {:<13} | {:<13} | {:<13} | {:<8}",
        "Chunk",
        "Downloaded",
        "Last Chunk",
        "AWS Upload",
        "First Radial",
        "Last Radial",
        "Attempts"
    );
    println!("{:-<128}", "");

    let config = PollConfig::new(&cli.site);
    let stream = chunk_stream(config);
    let mut stream = pin!(stream);

    let mut downloaded_chunk_count = 0;
    let mut last_chunk_time: Option<DateTime<Utc>> = None;

    // Stream chunks with a 5 minute overall timeout
    let result = timeout(Duration::from_secs(300), async {
        while let Some(result) = stream.next().await {
            match result {
                Ok(downloaded) => {
                    let download_time = Utc::now();
                    process_chunk(&downloaded, download_time, last_chunk_time);

                    last_chunk_time = downloaded.identifier.upload_date_time();

                    downloaded_chunk_count += 1;
                    if downloaded_chunk_count >= desired_chunk_count {
                        info!("Downloaded {} chunks, stopping...", desired_chunk_count);
                        break;
                    }
                }
                Err(e) => {
                    warn!("Error downloading chunk: {:?}", e);
                }
            }
        }
    })
    .await;

    match result {
        Ok(()) => info!("Finished downloading chunks"),
        Err(_) => info!("Timeout reached, stopping..."),
    }

    Ok(())
}

fn process_chunk(
    downloaded: &DownloadedChunk,
    download_time: DateTime<Utc>,
    last_chunk_time: Option<DateTime<Utc>>,
) {
    let chunk_id = &downloaded.identifier;
    let chunk = &downloaded.chunk;
    let attempts = downloaded.attempts;

    match chunk {
        Chunk::Start(file) => {
            let records = file.records().expect("records");
            debug!(
                "Volume start chunk with {} records. Header: {:?}",
                records.len(),
                file.header()
            );

            for record in records {
                process_record(chunk_id, record, download_time, last_chunk_time, attempts);
            }
        }
        Chunk::IntermediateOrEnd(record) => {
            debug!("Intermediate or end volume chunk.");
            process_record(
                chunk_id,
                record.clone(),
                download_time,
                last_chunk_time,
                attempts,
            );
        }
    }
}

fn process_record(
    chunk_id: &realtime::ChunkIdentifier,
    mut record: volume::Record,
    download_time: DateTime<Utc>,
    last_chunk_time: Option<DateTime<Utc>>,
    attempts: usize,
) {
    debug!("Decoding LDM record...");
    if record.compressed() {
        debug!("Decompressing LDM record...");
        record = record.decompress().expect("Failed to decompress record");
    }

    let messages = match record.messages() {
        Ok(msgs) => msgs,
        Err(e) => {
            warn!("Failed to decode messages: {e}");
            return;
        }
    };

    let summary = summarize::messages(messages.as_slice());

    // Calculate latencies
    let first_radial_latency = summary
        .earliest_collection_time
        .map(|time| (download_time - time).num_milliseconds() as f64 / 1000.0)
        .unwrap_or(f64::NAN);

    let last_radial_latency = summary
        .latest_collection_time
        .map(|time| (download_time - time).num_milliseconds() as f64 / 1000.0)
        .unwrap_or(f64::NAN);

    // AWS rounds to the second for object modified times
    let rounded_download_time = download_time.round_subsecs(0);

    let aws_latency = chunk_id
        .upload_date_time()
        .map(|time| {
            if rounded_download_time < time {
                warn!("Download time is before S3 modified time: download={download_time}, rounded download={rounded_download_time}, s3={time}");
            }

            (rounded_download_time - time).num_milliseconds() as f64 / 1000.0
        })
        .unwrap_or(f64::NAN);

    // Compare chunk_id.date_time() with last_chunk_time, though either could be None
    let time_since_last_chunk = match (chunk_id.upload_date_time(), last_chunk_time) {
        (Some(current), Some(last)) => {
            format!("{}", (current - last).num_milliseconds() as f64 / 1000.0)
        }
        _ => String::from("N/A"),
    };

    // Print concise output in a single line
    println!(
        "{:<25} | {:<25} | {:<12}s | {:<12}s | {:<12}s | {:<12}s | {:<8}",
        format!("{}/{}", chunk_id.volume().as_number(), chunk_id.name()),
        download_time.format("%Y-%m-%d %H:%M:%S%.3f"),
        time_since_last_chunk,
        aws_latency,
        first_radial_latency,
        last_radial_latency,
        attempts
    );
}
